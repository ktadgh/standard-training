{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/k-diff-dist/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import k_diffusion as K\n",
    "import torch._C._onnx as _C_onnx\n",
    "from torchinfo import summary\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "import pynvml\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.079396\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "torch.Size([1, 1024, 1024, 3])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     21\u001b[0m cst \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): out \u001b[38;5;241m=\u001b[39m final_activation_function(model(img, cst)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m.5\u001b[39m\n\u001b[1;32m     25\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/k-diff-dist/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/k-diff-dist/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/transformer-distillation/k-diffusion-onnx/k_diffusion/models/image_transformer_v2.py:794\u001b[0m, in \u001b[0;36mImageTransformerDenoiserModelV2.forward\u001b[0;34m(self, x, sigma, aug_cond, class_cond, mapping_cond)\u001b[0m\n\u001b[1;32m    792\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm(x)\n\u001b[1;32m    793\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_out(x)\n\u001b[0;32m--> 794\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    795\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: torch.Size([1, 1024, 1024, 3])"
     ]
    }
   ],
   "source": [
    "config = \"/home/ubuntu/transformer-distillation/configs/hdit-shifted-windows/head1.json\"\n",
    "config = K.config.load_config(config)\n",
    "\n",
    "model = K.config.make_model(config).cuda()\n",
    "\n",
    "# dct = torch.load(\"200_net_G_hdit.pth\")#.keys()\n",
    "# dct = {\n",
    "#     key.replace('model.', ''): value for key, value in dct.items()\n",
    "# }\n",
    "\n",
    "# model.load_state_dict(dct)\n",
    "final_activation_function = nn.Tanh()\n",
    "print(sum([p.numel() for p in model.parameters()]) / 1e6)\n",
    "model.eval()\n",
    "\n",
    "img = torch.from_numpy(np.array(Image.open(\"input_0_24_62.png\")))\n",
    "img = img / 255.\n",
    "img = img * 2. - 1.\n",
    "img = img.transpose(-1, -2).transpose(-2, -3).unsqueeze(0).cuda()\n",
    "\n",
    "cst = torch.ones((img.shape[0])).cuda() * 500\n",
    "with torch.no_grad(): out = final_activation_function(model(img, cst)[0])\n",
    "\n",
    "out = out / 2. + .5\n",
    "out = out.squeeze(0).transpose(0, 1).transpose(1, 2)\n",
    "out = Image.fromarray((out.data.cpu().numpy() * 255.).astype(np.uint8))\n",
    "out.save('out_shifted.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, model, device):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Example: simple linear layer\n",
    "        self.cst = torch.ones((1)).to(device) *500\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, img):\n",
    "        x  = self.model(img,self.cst)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/transformer-distillation/k-diffusion-onnx/k_diffusion/models/axial_rope.py:54: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  y_min = torch.max(torch.tensor(-1), -1 / ar_adj)\n",
      "/home/ubuntu/transformer-distillation/k-diffusion-onnx/k_diffusion/models/axial_rope.py:55: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  y_max = torch.min(torch.tensor(1), 1 / ar_adj)\n",
      "/home/ubuntu/transformer-distillation/k-diffusion-onnx/k_diffusion/models/axial_rope.py:57: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x_min = torch.max(torch.tensor(-1), -ar_adj)\n",
      "/home/ubuntu/transformer-distillation/k-diffusion-onnx/k_diffusion/models/axial_rope.py:58: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x_max = torch.min(torch.tensor(1), ar_adj)\n",
      "/home/ubuntu/transformer-distillation/k-diffusion-onnx/k_diffusion/models/image_transformer_v2.py:457: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  nh = int(t_nh_e / (t*e))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "pmodel = MyModel(model, 'cpu')\n",
    "of = 'ps1.onnx'\n",
    "input = torch.tensor(cv2.imread('input_0_24_62.png')).permute(2,0,1).unsqueeze(0).cuda()/255.\n",
    "torch.onnx.export(pmodel.cpu(), (input.cpu()), of, opset_version = 17,do_constant_folding=True, export_params=True,input_names = ['input'],output_names = ['output'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/OMGD/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258.6513671875\n"
     ]
    }
   ],
   "source": [
    "providers = [\n",
    "    ('CUDAExecutionProvider')]\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "sess_options.enable_mem_pattern = False\n",
    "sess_options.use_deterministic_compute = True\n",
    "sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "sess_options.enable_cpu_mem_arena = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "session = ort.InferenceSession(\"ps1.onnx\",sess_options,providers=providers)\n",
    "\n",
    "# Run inference with ONNX Runtime\n",
    "\n",
    "\n",
    "input_data = input.cpu().numpy().astype(np.float32)\n",
    "ort_inputs = {session.get_inputs()[0].name: input_data}\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "for _ in range(5):\n",
    "    start.record()\n",
    "    onnx_output = session.run(None, ort_inputs)\n",
    "    end.record()\n",
    "    print(start.elapsed_time(end))\n",
    "\n",
    "# Compare the outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k-diff-dist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
