{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tdist-flat/tdist-regular/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"K_DIFFUSION_USE_COMPILE\"] = \"0\"\n",
    "\n",
    "import k_diffusion as K\n",
    "import torch._C._onnx as _C_onnx\n",
    "from torchinfo import summary\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "import pynvml\n",
    "# import onnxruntime as ort\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2350966/3516763265.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dct = torch.load(\"/home/ubuntu/tdist-flat/pix2pixHD/checkpoints/og-reg-4/epoch_18_netG.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected keys: dict_keys(['alpha1', 'alpha2', 'alpha3', 'alpha4', 'alpha5', 'alpha_sum', 'projector1.weight', 'projector2.weight', 'projector3.weight', 'projector4.weight', 'projector5.weight'])\n",
      "85.087492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2350966/3516763265.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img = torch.load(\"/home/ubuntu/label.pt\")[0].cuda().unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "config = \"/home/ubuntu/tdist-flat/configs/hdit_shifted_window.json\"\n",
    "config = K.config.load_config(config)\n",
    "\n",
    "model = K.config.make_model(config).cuda()\n",
    "\n",
    "dct = torch.load(\"/home/ubuntu/tdist-flat/pix2pixHD/checkpoints/og-reg-4/epoch_18_netG.pth\")\n",
    "dct = {key.replace('model.', ''): value for key, value in dct.items()}\n",
    "\n",
    "# Find unexpected keys\n",
    "model_keys = set(model.state_dict().keys())\n",
    "unexpected_keys = {key: value for key, value in dct.items() if key not in model_keys}\n",
    "\n",
    "# Print or log the unexpected keys\n",
    "print(\"Unexpected keys:\", unexpected_keys.keys())\n",
    "\n",
    "# Optionally, you can remove them before loading the state_dict\n",
    "filtered_dct = {key: value for key, value in dct.items() if key in model_keys}\n",
    "\n",
    "# Load the filtered state_dict\n",
    "model.load_state_dict(filtered_dct, strict=False)\n",
    "\n",
    "final_activation_function = nn.Tanh()\n",
    "print(sum([p.numel() for p in model.parameters()]) / 1e6)\n",
    "model.eval()\n",
    "\n",
    "img = torch.load(\"/home/ubuntu/label.pt\")[0].cuda().unsqueeze(0)\n",
    "\n",
    "cst = torch.ones((img.shape[0])).cuda() * 500\n",
    "with torch.no_grad(): out = final_activation_function(model(img, cst)[0])\n",
    "\n",
    "out = out / 2. + .5\n",
    "# raise ValueError(out.shape)\n",
    "out = out.squeeze(0).transpose(0, 1).transpose(1, 2)\n",
    "out = Image.fromarray((out.data.cpu().numpy() * 255.).astype(np.uint8))\n",
    "out.save('out_shifted.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, model, device):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Example: simple linear layer\n",
    "        self.cst = torch.ones((1)).to(device) *500\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, img):\n",
    "        x  = self.model(img,self.cst)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tdist-flat/kdiff-older/k_diffusion/models/axial_rope.py:54: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  y_min = torch.max(torch.tensor(-1), -1 / ar_adj)\n",
      "/home/ubuntu/tdist-flat/kdiff-older/k_diffusion/models/axial_rope.py:55: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  y_max = torch.min(torch.tensor(1), 1 / ar_adj)\n",
      "/home/ubuntu/tdist-flat/kdiff-older/k_diffusion/models/axial_rope.py:57: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x_min = torch.max(torch.tensor(-1), -ar_adj)\n",
      "/home/ubuntu/tdist-flat/kdiff-older/k_diffusion/models/axial_rope.py:58: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x_max = torch.min(torch.tensor(1), ar_adj)\n",
      "/home/ubuntu/tdist-flat/kdiff-older/k_diffusion/models/image_transformer_v2.py:456: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  nh = int(t_nh_e / (t*e))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "z_(): incompatible function arguments. The following argument types are supported:\n    1. (self: torch._C.Node, arg0: str, arg1: torch.Tensor) -> torch._C.Node\n\nInvoked with: %976 : Tensor = onnx::Constant(), scope: __main__.MyModel::/k_diffusion.models.image_transformer_v2.ImageTransformerDenoiserModelV2::model/k_diffusion.models.image_transformer_v2.Level::down_levels.0/k_diffusion.models.image_transformer_v2.ShiftedWindowTransformerLayer::down_levels.0.0/k_diffusion.models.image_transformer_v2.ShiftedWindowSelfAttentionBlock::self_attn\n, 'value', 1.0 \n(Occurred when translating scaled_dot_product_attention).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# input = torch.load(\"/home/ubuntu/transformer-distillation/tensor_14.pt\").unsqueeze(0).cuda()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# input = input.permute(0,2,3,1)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m1024\u001b[39m,\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mof\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/utils.py:551\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExport destination must be specified for torchscript-onnx export.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    549\u001b[0m     )\n\u001b[0;32m--> 551\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/utils.py:1648\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1645\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1646\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1648\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1663\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1664\u001b[0m )\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/utils.py:1174\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1171\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1174\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1185\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/utils.py:714\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    711\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[1;32m    712\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 714\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    716\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/utils.py:1997\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, values_in_env, new_nodes, operator_export_type)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m symbolic_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1993\u001b[0m         \u001b[38;5;66;03m# TODO Wrap almost identical attrs assignment or comment the difference.\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m         attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1995\u001b[0m             k: symbolic_helper\u001b[38;5;241m.\u001b[39m_node_get(node, k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattributeNames()\n\u001b[1;32m   1996\u001b[0m         }\n\u001b[0;32m-> 1997\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msymbolic_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1999\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2000\u001b[0m     k \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mkindOf(k)[\u001b[38;5;241m0\u001b[39m]: symbolic_helper\u001b[38;5;241m.\u001b[39m_node_get(node, k)\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattributeNames()\n\u001b[1;32m   2002\u001b[0m }\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;66;03m# Clone node to trigger ONNX shape inference\u001b[39;00m\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/symbolic_helper.py:292\u001b[0m, in \u001b[0;36mparse_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(g, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs, (\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbolic function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**kwargs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only contain \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**kwargs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFILE_BUG_MSG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/symbolic_opset14.py:177\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(g, query, key, value, attn_mask, dropout_p, is_causal, scale)\u001b[0m\n\u001b[1;32m    173\u001b[0m key_transposed \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, key, perm_i\u001b[38;5;241m=\u001b[39mkey_transposed_axes)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/blob/12da0c70378b5be9135c6fda62a9863bce4a4818/aten/src/ATen/native/transformers/attention.cpp#L653\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Scale q, k before matmul for stability see https://tinyurl.com/sudb9s96 for math\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m query_scaled \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMul\u001b[39m\u001b[38;5;124m\"\u001b[39m, query, \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSqrt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    178\u001b[0m key_transposed_scaled \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMul\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_transposed, g\u001b[38;5;241m.\u001b[39mop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSqrt\u001b[39m\u001b[38;5;124m\"\u001b[39m, scale))\n\u001b[1;32m    179\u001b[0m mul_qk \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatMul\u001b[39m\u001b[38;5;124m\"\u001b[39m, query_scaled, key_transposed_scaled)\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:93\u001b[0m, in \u001b[0;36mGraphContext.op\u001b[0;34m(self, opname, outputs, *raw_args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an ONNX operator \"opname\", taking \"raw_args\" as inputs and \"kwargs\" as attributes.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03mThe set of operators and the inputs/attributes they take\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    keyword argument for multi-return nodes).\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# FIXME(justinchuby): Add the return type back once we know how to handle mypy\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_add_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mraw_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:244\u001b[0m, in \u001b[0;36m_add_op\u001b[0;34m(graph_context, opname, outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_op\u001b[39m(\n\u001b[1;32m    207\u001b[0m     graph_context: GraphContext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ):\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates an ONNX operator \"opname\", taking \"args\" as inputs and attributes \"kwargs\".\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    The set of operators and the inputs/attributes they take\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m        keyword argument for multi-return nodes).\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [_const_if_tensor(graph_context, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Filter out None attributes, this can be convenient client side because\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# now they can pass through None attributes, and have them not show up\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:244\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_op\u001b[39m(\n\u001b[1;32m    207\u001b[0m     graph_context: GraphContext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ):\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates an ONNX operator \"opname\", taking \"args\" as inputs and attributes \"kwargs\".\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    The set of operators and the inputs/attributes they take\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m        keyword argument for multi-return nodes).\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\u001b[43m_const_if_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Filter out None attributes, this can be convenient client side because\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# now they can pass through None attributes, and have them not show up\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:276\u001b[0m, in \u001b[0;36m_const_if_tensor\u001b[0;34m(graph_context, arg)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, _C\u001b[38;5;241m.\u001b[39mValue):\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_add_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monnx::Constant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:252\u001b[0m, in \u001b[0;36m_add_op\u001b[0;34m(graph_context, opname, outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m opname:\n\u001b[1;32m    250\u001b[0m     opname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx::\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m opname\n\u001b[0;32m--> 252\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[43m_create_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGLOBALS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_shape_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m graph_context\u001b[38;5;241m.\u001b[39mnew_nodes\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:312\u001b[0m, in \u001b[0;36m_create_node\u001b[0;34m(graph_or_block, domain_op, inputs, attributes, params_dict, opset_version, n_outputs, shape_inference)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m _SKIP_NODE_ATTRIBUTES:\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     \u001b[43m_add_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maten\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape_inference:\n\u001b[1;32m    314\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "File \u001b[0;32m~/tdist-flat/tdist-regular/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:363\u001b[0m, in \u001b[0;36m_add_attribute\u001b[0;34m(node, key, value, aten)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m             kind \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkind\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: z_(): incompatible function arguments. The following argument types are supported:\n    1. (self: torch._C.Node, arg0: str, arg1: torch.Tensor) -> torch._C.Node\n\nInvoked with: %976 : Tensor = onnx::Constant(), scope: __main__.MyModel::/k_diffusion.models.image_transformer_v2.ImageTransformerDenoiserModelV2::model/k_diffusion.models.image_transformer_v2.Level::down_levels.0/k_diffusion.models.image_transformer_v2.ShiftedWindowTransformerLayer::down_levels.0.0/k_diffusion.models.image_transformer_v2.ShiftedWindowSelfAttentionBlock::self_attn\n, 'value', 1.0 \n(Occurred when translating scaled_dot_product_attention)."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "pmodel = MyModel(model, 'cuda')\n",
    "pmodel = pmodel\n",
    "of = '/home/ubuntu/og-ps4.onnx'\n",
    "# input = torch.load(\"/home/ubuntu/transformer-distillation/tensor_14.pt\").unsqueeze(0).cuda()\n",
    "# input = input.permute(0,2,3,1)\n",
    "input = torch.rand(1,7,1024,1024).cpu()\n",
    "torch.onnx.export(pmodel.cuda(), (input.cuda()), of,do_constant_folding=True, export_params=True,input_names = ['input'],output_names = ['output'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'channels-last.onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the ONNX model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels-last.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Perform shape inference\u001b[39;00m\n\u001b[1;32m      9\u001b[0m inferred_model \u001b[38;5;241m=\u001b[39m shape_inference\u001b[38;5;241m.\u001b[39minfer_shapes(model)\n",
      "File \u001b[0;32m~/tdist-flat/tdist-flat/lib/python3.10/site-packages/onnx/__init__.py:210\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(f, format, load_external_data)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\n\u001b[1;32m    190\u001b[0m     f: IO[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike,\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mformat\u001b[39m: _SupportedFormat \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: A002\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     load_external_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    193\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelProto:\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a serialized ModelProto into memory.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        Loaded in-memory ModelProto.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     model \u001b[38;5;241m=\u001b[39m _get_serializer(\u001b[38;5;28mformat\u001b[39m, f)\u001b[38;5;241m.\u001b[39mdeserialize_proto(\u001b[43m_load_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m, ModelProto())\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load_external_data:\n\u001b[1;32m    213\u001b[0m         model_filepath \u001b[38;5;241m=\u001b[39m _get_file_path(f)\n",
      "File \u001b[0;32m~/tdist-flat/tdist-flat/lib/python3.10/site-packages/onnx/__init__.py:147\u001b[0m, in \u001b[0;36m_load_bytes\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     f \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mcast(Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], f)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m readable:\n\u001b[1;32m    148\u001b[0m         content \u001b[38;5;241m=\u001b[39m readable\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'channels-last.onnx'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import shape_inference\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = 'channels-last.onnx'\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# Perform shape inference\n",
    "inferred_model = shape_inference.infer_shapes(model)\n",
    "\n",
    "# Save the inferred model (optional)\n",
    "onnx.save(inferred_model, 'channels-last-inferred.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.0793\n"
     ]
    }
   ],
   "source": [
    "print(sum([p.numel() for p in pmodel.parameters()]) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361958/3462181165.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  input = torch.load(\"/home/ubuntu/transformer-distillation/tensor_14.pt\").unsqueeze(0).cuda()\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/transformer-distillation/tensor_14.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/transformer-distillation/tensor_14.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m providers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPUExecutionProvider\u001b[39m\u001b[38;5;124m'\u001b[39m, {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m# The device ID\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     })\n\u001b[1;32m     12\u001b[0m ]\n",
      "File \u001b[0;32m~/tdist-flat/tdist-flat/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/tdist-flat/tdist-flat/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/tdist-flat/tdist-flat/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/transformer-distillation/tensor_14.pt'"
     ]
    }
   ],
   "source": [
    "input = torch.load(\"/home/ubuntu/transformer-distillation/tensor_14.pt\").unsqueeze(0).cuda()\n",
    "input = input.permute(0,2,3,1)\n",
    "\n",
    "providers = [\n",
    "    ('CPUExecutionProvider', {\n",
    "        'device_id': 0, # The device ID\n",
    "        # 'trt_max_workspace_size': 4e9, # Maximum workspace size for TensorRT engine (1e9 ≈ 1GB)\n",
    "        # 'trt_engine_cache_enable': False, # Enable TensorRT engine caching\n",
    "        # # 'trt_engine_cache_path': str(trt_cache_dir), # Path for TensorRT engine, profile files, and int8 calibration table\n",
    "        # 'trt_int8_enable': True, # Enable int8 mode in TensorRT\n",
    "    })\n",
    "]\n",
    "\n",
    "# providers = \n",
    "#     ('CUDAExecutionProvider')]\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "sess_options.enable_mem_pattern = False\n",
    "sess_options.use_deterministic_compute = True\n",
    "sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "sess_options.enable_cpu_mem_arena = False\n",
    "\n",
    "\n",
    "\n",
    "session = ort.InferenceSession(\"channels-last-inferred.onnx\",sess_options,providers=providers)\n",
    "\n",
    "# Run inference with ONNX Runtime\n",
    "input_data = input.cpu().numpy().astype(np.float32)\n",
    "ort_inputs = {session.get_inputs()[0].name: input_data}\n",
    "# start = torch.cuda.Event(enable_timing=True)\n",
    "# end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # start.record()\n",
    "onnx_output = session.run(None, ort_inputs)\n",
    "    # end.record()\n",
    "    # print(start.elapsed_time(end))\n",
    "# Compare the outputs\n",
    "\n",
    "# outim = np.transpose(out[0].data.cpu().numpy(), (1,2,0))\n",
    "outim = out[0].data.cpu().numpy()\n",
    "out = Image.fromarray((outim * 255.).astype(np.uint8))\n",
    "out.save('out_shifted1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx2keras\n",
    "from onnx2keras import onnx_to_keras\n",
    "import keras\n",
    "import onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.tanh(torch.tensor(onnx_output[0]))\n",
    "out = out / 2. + .5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Image.fromarray((outim * 255.).astype(np.uint8))\n",
    "out.save('out_shifted1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
